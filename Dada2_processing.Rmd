---
title: "Dada2_processing"
author: "Vinita Periwal"
date: "`r Sys.Date()`"
output: 
  
  html_document:
    code_folding: hide
    fig_caption: true
    toc: true
    toc_float:
          collapsed: false
          smooth_scroll: false
  editor_options:
    chunk_output_type: console
---

<h4><i>Sweeteners 16S community sequencing</i></h4>
<h4><i></i></h4>

<style>
p.caption {
  font-size: 1.2em;
}

.table caption {
    font-size: 1.2em;
}
</style>

##loading packages

loading the required packages.

```{r echo=TRUE, setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      cache = FALSE,
                      tidy = FALSE,
                      size = "small",
                      fig.width = 10,
                      fig.height = 8
)

library(dada2)
library(DECIPHER)
library(ShortRead)
library(Biostrings)
library(dplyr)
library(tidyr)
library(ggplot2)
library(readr)
library(stringr)
library(phyloseq)

set.seed(100)

```

# Reading demultiplexed files and primers/adapters

The 16S sequencing data comprise of paired-end sequencing reads from V4 region of the bacterial genomes.Plot qualtiy of first few reads and identify trimming parameters. The adapters were checked with cutadapt tool but there were no adaptors found.

```{r, echo=TRUE}
knitr::opts_knit$set(root.dir = '/Users/periwal/ShikiFactory/WP3/SW_SF100/Community/')

#reading reads

path<-"./Community/sweet_community/16Sreads"
list.files(path)

forward_reads<-sort(list.files(path, pattern = "_1_sequence.txt.gz", full.names = TRUE))
reverse_reads<-sort(list.files(path, pattern = "_2_sequence.txt.gz", full.names = TRUE))

print(plotQualityProfile(forward_reads[1:2]))
print(plotQualityProfile(reverse_reads[1:2]))

#adaptor
FWD <- "CTCTTTCCCTACACGACGCTCTTCCGATCT"
REV <- "CTGGAGTTCAGACGTGTGCTCTTCCGATCT"

#get all orientation
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

#The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

fnFs.filtN <- file.path(path, "filtN", basename(forward_reads)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(reverse_reads))
filterAndTrim(forward_reads, fnFs.filtN, reverse_reads, fnRs.filtN, maxN = 0, multithread = TRUE)

#identify/count the primers where they occur
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

#takes some time
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

```

# Remove Primers

Install cutadapat if you don’t have it already. After installing cutadapt, we need to tell R the path to the cutadapt command.

```{r, echo=TRUE}
cutadapt <- "/Users/periwal/opt/anaconda3/envs/qiime2-2022.8/bin/cutadapt"
system2(cutadapt, args = "--version")

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_1_sequence")[[1]][1]
sample.names <- unname(sapply(fnFs.filtN, get.sample.name))
head(sample.names)

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

# Create an output directory to store the clipped files
cut_dir <- file.path(path, "cutadapt")
if (!dir.exists(cut_dir)) dir.create(cut_dir)

fwd_cut <- file.path(cut_dir, basename(fnFs.filtN))
rev_cut <- file.path(cut_dir, basename(fnRs.filtN))

names(fwd_cut) <- sample.names
names(rev_cut) <- sample.names

# Create log files for console output
cut_logs <- path.expand(file.path(cut_dir, paste0(sample.names, ".log")))

# Run Cutadapt
for(i in seq_along(fnFs.filtN)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
        "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
        fnFs.filtN[i], fnRs.filtN[i]), # input files
        stdout = cut_logs[i]) 
}

```

# Filter and trim reads

From the above plots of forward and reverse reads, a trim length of 245bp for forwards reads and 247bp for reverse reads is selected based on quality score of 30. We’ll use standard filtering parameters: maxN=0 (DADA2 requires no Ns).

```{r, echo=TRUE}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

#plot quality of trimmed reads
print(plotQualityProfile(fwd_cut[1:2]))
print(plotQualityProfile(rev_cut[1:2]))

out <- data.frame(filterAndTrim(fwd_cut, filtFs, rev_cut, filtRs,
                                truncLen=c(230,240), maxN=0, maxEE=c(2,2),
                                multithread=TRUE, matchIDs=TRUE))
head(out)

#percentage of reads filtered
out_percent<-out %>% mutate(percent_filtered = reads.out/reads.in)
head(out_percent)

```

# Learn error rates

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution.

```{r, echo=TRUE}
errF <- learnErrors(filtFs, multithread=TRUE)
#107539950 total bases in 467565 reads from 7 samples will be used for learning the error rates.
errR <- learnErrors(filtRs, multithread=TRUE)
#112215600 total bases in 467565 reads from 7 samples will be used for learning the error rates.

#plot errors
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)

```

# Sample inference

```{r, echo=TRUE}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

dadaFs[[20]]
dadaRs[[20]]

```

# Merge paired reads
We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region.

```{r, echo=TRUE}

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])

```

# Construct sequence table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r, echo=TRUE}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

```

# Remove chimeras
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r, echo=TRUE}
#used 'per-sample' method to remove chimeras, other methods discards majority of the sequences as chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="per-sample", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
head(seqtab.nochim)
sum(seqtab)
sum(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)

```

# Track reads through the pipeline
As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r, echo=TRUE}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim), rowSums(seqtab.nochim)/sapply(mergers, getN))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim","%")
rownames(track) <- sample.names
head(track)

```

# Assign taxonomy

```{r, echo=TRUE}
ref<-"~/ShikiFactory/WP3/SW_SF100/Community/sweet_community/16S_25bugs.fa"

taxa <- assignTaxonomy(seqtab.nochim, ref, multithread = TRUE, tryRC = TRUE)
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

#sample metadata
samples.out <- rownames(seqtab.nochim)
samples.out

samdf <- data.frame(read.table(file = '~/ShikiFactory/WP3/SW_SF100/Community/sweet_community/metadata_sampbiol.tsv', header = TRUE, sep = '\t'))
rownames(samdf) <- samples.out
head(samdf)

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps

#save files
asv_table<-data.frame(t(otu_table(ps)))
#View(asv_table)
df0 <- tibble::rownames_to_column(asv_table, "ASV")
write.table(df0, file = "~/ShikiFactory/WP3/SW_SF100/Community/sweet_community/ASV_25bugs.tsv", sep = '\t', row.names = FALSE)

taxa_table<-data.frame(tax_table(ps))
df <- tibble::rownames_to_column(taxa_table, "ASV")
head(df)
write.table(df, file = "~/ShikiFactory/WP3/SW_SF100/Community/sweet_community/TAXA_25bugs.tsv", sep = '\t', row.names = FALSE)

# plot_richness(ps, x="samp_biolrep", measures="Shannon", color="compound")
# 
# # Transform data to proportions as appropriate for Bray-Curtis distances
# ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
# ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
# 
# plot_ordination(ps.prop, ord.nmds.bray, color="treatment", title="Bray NMDS")

```